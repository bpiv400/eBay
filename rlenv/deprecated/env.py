'''
Module defining training environment for an experiment

Classes:
    ActorCriticEnvironment: encapsulates functionality related to
    loading simulator, initializing and training actor, and
    interacting with listing environment
'''
import json
import torch
import numpy as np
from deprecated.listings import ListingEnvironment
from util import actor_map, simulator_map, is_none


class ActorCriticEnvironment:
    '''
    NOTES TO ETAN

    1. Place simulator experiments in subdirectories of
    data/sims folder. For each simulator, initialize a
    params.json file (see apinotes for details)

    2. Assumptions about simulator and reinforcement learner offer inputs:
        - First s_o indices of reinforcement learner offer input vector consist of features
        generated by last seller (simulator) offer...currently:
            - 0 - indicator for whether a message is given
            - 1 - indicator for whether offer is 50 / 50
            - 2 - normalized value of offer
            - 3 - normalized buyer delay
            - 4 - real value of offer (rounded)
            - 5 - real value of delay (secs) (rounded?)
        - First s_1 indices of simulator offer input vector consist of features generated
        by the last seller (simulator) offer...currently: see above
        - Middle b_o indices of reinforcement learner offer input vector consist of features
        generated by the last buyer (rl) offer...currently: see above
        - Middle b_1 indices of simulator offer input vector consist of features
        generated by the last buyer (rl) offer...currently: see above
        - Last k_o indices of reinforcement learner offer input vector consist of time
        valued features
        - Last k_1 indices of simulator offer input vector consist of time valued features

    3. Currently, the buyer does not incur the cost of bargaining if the item is sold before the
    offer is placed. And the buyer does incur the cost of bargaining for rejections and acceptances

    TODO / BUG: (use best judgement for order)
    5. Finish mini-batch training code
    6. Create fake input files as necessary
    6a. figure out how to test drive with simulator
    7. DEBUG env.py, listing.py, actor.py
    8. Handle automatic acceptance/rejection
    9. Create baseline/critic model
    8. Finalize tree and publish gist
    10. Find a better way to accumulate list of listings (parallelizable or after generate_listing)
    11. Modify everything to handle constant buyer features as parameters
    12. Consider modifying listing to expect torch inputs & store data directly as torches #!
    13. Update simulator to handle step, init_hidden, and shrink_hidden functionalities
    14. Extend actor to include last step (strictly accept/reject functionality)
    Class for encapsulating training environment of reinforcement learner

    Args:
        trans_prob_name : string giving the name of the simulator to use
        exp_name : string giving the name of the reinforcement learning experiment
    Required resources:
        -See apinotes
    Required functionality:
        -Load experiment JSON and parse parameters from it
        -Load simulator
        -Initialize actor and critic
        -Train actor:
            - For each mini-batch
            - Extract n listings from listing data structure
            - For each bargaining round:
                - Convert vector of seller actions to actor/critic features, as necessary
                - Query TimedFeature data structure for timed feature vector
                 for each thread in minibatch at input time for actor
                - Combine these time features with vector of seller features
                 (if any exists) to pass to actor
                - Round concessions to amounts observed in the data
                - Get action and log probabilities from actor
                - Evaluate state value with critic
                - Store log probabilities of actions somewhere
                - For all accepts and rejects, populate remainder of action
                 sequences with empty values
                - Tell the simulator to ignore those indices going forward
                - Convert vector of buyer actions to simulator features, as necessary
                - Query TimeFeature data structure for timed feature vector for
                 each thread in minibatch at input time for seller simulation
                - Combine these time features with vector of buyer features (if any exists)
                - Round concessions to amounts observed in the data? (Unsure if necessary)
                - Pass feature vector built above to simulator and receive counter offers
                - If the simulator rejects (at least on the last turn),
                 then tell the actor to ignore going forward
            - Compute the reward for each sequence of offers
            - Compute the REINFORCE objective function
            - Store average reward
            - back-propogate over network
        - Save trained actor and loss metrics and curves

    - TODO: Treat constant buyer features as parameters
    '''
    BUYER_TIME = 48 * 60 * 60  # amount of time buyer may delay for
    SELLER_TIME = 14 * 24 * 60 * 60  # amount of time seller may delay for
    PREV_FEATS = 6
    # maximum number of pairs of offers (buyer makes max_rounds + 1 offers at most)
    MAX_ROUNDS = 3

    def __init__(self, exp_name=None):
        '''
        Initialize parameters from json files and instantiate
        ListingEnv, Actor, and Critic
        '''

        # error checking
        is_none(exp_name)
        # store experiment name as a local variable
        self.exp_name = exp_name
        # initialize empty simulator and rl parameter dictionaries
        self.rl_params = {}
        self.sim_params = {}
        # initialize null actor and simulator
        self.sim = None
        self.actor = None
        # initialize null log probability structure for actions
        self.lprobs = None
        # initialize null rewards structure
        self.rewards = None
        # initialize null delays structure
        self.delays = None
        self.delay_tracker = None
        # initialize null included indices
        self.included = None
        self.included_list = []
        # initialize null sale times & prices
        self.sale_times = None
        self.sale_prices = None
        self.init_prices = None
        # extract params from experiment name (directly for rl)
        # and through 'sim_name' field for simulator
        self.__get_params()
        # initialize transition probability model
        self.__init_simulator()
        # init actor
        self.__init_actor()
        # init listing environment class
        self.listing_env = ListingEnvironment.load(self.rl_params['data_name'])
        #
        # for the time being, assume no baseline
        # init critic
        # self.__init_critic(self.exp_name)

    def train_batch(self, listing_ids=None):
        """
        See class docstring for specific functionality

        Kwargs:
            listing_ids: 1-dimensional np.array giving listing ids
            for listings in the current batch. Default = None
        """
        # error checking
        is_none(listing_ids)
        self.gen_seqs(listing_ids)  # generate sequences for each

    def __init_actor(self):
        '''
        Private method that initializes actor model using exp_params
        '''
        actor_class = actor_map()[self.rl_params['actor_class']]
        self.actor = actor_class(self.rl_params['n_fixed'],
                                 self.rl_params['n_hidden'],
                                 self.rl_params['n_offer'],
                                 self.rl_params['n_layer'])

    # def __init_critic(self, exp_name):
    #     '''
    #     Private method that initializes critic model using exp_params
    #
    #     Out of commission presently
    #     '''
    #     CriticClass = ActorCriticEnvironment.get_critic_class(self.exp_name)
    #     self.critic = CriticClass(self.exp_params['org_hidden_size'],
    #                               self.exp_params['targ_hidden_size'],
    #                               self.exp_params['num_offr_feats'],
    #                               self.exp_params['num_layers'])

    def remove_threads(self, act, turn, sim=False):
        """
        Determines which indices should remain included in the batch
        of sequences (i.e. which offers have not been accepts/rejects)
        in the most recent round

        Args:
            included: 1-dimensional tensor containing indices of batch elements for
            which the most recent offer was executed
            act: 2-dimensional tensor containing action output of simulator/rl
            turn:

        Returns:
            tuple containing three elements, where the first is a 1-dimensionl
            byte tensor giving a boolean for whether each index should be kept,
            the second is a 1-dimensional byte tensor giving whether each index
            has been rejected/accepted, and the third is a 1-dimensional byte tensor
            giving whether each index has timed out (i.e. sold before the action is
            executed given the delay)
        """
        # extract slice where nan implies accept/reject
        ind_slice = act[:, 0]
        # compute offer number corresponding to the offer just made
        offr_num = turn * 2 + 1 if sim else turn * 2
        # elementwise determine which indices conform to
        acc_rejs = torch.isnan(ind_slice).byte()
        keeps = acc_rejs ^ 1
        # compute new delays
        # set multiplier
        if sim:
            modifier = self.SELLER_TIME
        else:
            modifier = self.BUYER_TIME
        curr_delay = act[:, 3]
        # convert to seconds
        curr_delay = curr_delay * modifier
        # store current delays
        if turn != 3:
            turn = turn * 2 + 1 if sim else turn * 2
            self.delay_tracker[self.included, turn] = curr_delay
        self.delays = self.delays + curr_delay
        # determine which threads have timed out
        # and convert to tensor
        timeouts = self.delays >= self.sale_times
        timeouts = timeouts.byte()
        # remove the timeout indices from those which are kept
        keeps[timeouts] = 0
        # update accept / reject indices to exclude those which timed out
        # (timeout has precedence)
        acc_rejs[timeouts] = 0
        # update the delays maintained
        self.delays = self.delays[keeps]
        self.sale_times = self.sale_times[keeps]
        return keeps, acc_rejs, timeouts

    def gen_seqs(self, listing_ids):
        '''
        #! WARNING: Expect problems when all sequences end early
        Generate sequences using current policy and
        trans probs model
        1. Ensure a policy model exists
        2. Randomly samples constant features of model
        3. Transforms constant features to input expected by
        '''
        if self.sim is None or self.actor is None:
            raise ValueError("Must initialize transition model and actor before" +
                             "generating sequences")
        # initialize instance variables for batch below
        # initialize tensor to track offer prices
        self.offrs = torch.zeros((listing_ids.size, 6)).float()
        # initialize log probabilities
        self.lprobs = torch.zeros(
            (listing_ids.size, 3), requires_grad=True).float()
        # initialize rewards
        self.rewards = torch.zeros((listing_ids.size, 3)).float()
        # initialize listing objects for each
        self.listing_env.load_batch(listing_ids)
        # initialize list of cumulative delays and tensor giving
        # real delay for each turn (in seconds)
        self.delays = torch.zeros(listing_ids.size)
        self.delay_tracker = torch.zeros((listing_ids.size, 6))
        # get listing prices
        self.init_prices = torch.from_numpy(
            self.listing_env.get_bin_price()).float()
        # get sale prices
        self.sale_prices = torch.from_numpy(
            self.listing_env.get_sale_price()).float()
        # get sale times
        self.sale_times = torch.from_numpy(
            self.listing_env.get_sale_time()).float()
        # intialize list of included indices
        self.included = torch.from_numpy(np.arange(listing_ids.size)).long()
        self.included_list.append(self.included)
        # initialize rl hidden state as numpy matrices
        rl_init = self.listing_env.init(sim=False)
        # initialize sim hidden state as numpy matrices
        sim_init = self.listing_env.init(sim=True)
        # convert to tensors
        rl_init = torch.from_numpy(rl_init).float()
        sim_init = torch.from_numpy(sim_init).float()
        # initialize hidden state for actor and simulator
        self.actor.init_hidden_state(rl_init, rl_init)
        self.sim.init_hidden_state(sim_init, sim_init)
        # initialize prev rl action and previous simulator action to
        # none
        rlact = None
        simact = None
        # TODO: Figure out difference between from_numpy and other methods
        # loop while it's possible for both parties to make an offer
        for i in range(self.MAX_ROUNDS):
            # check whether all sequences have concluded
            if self.included.nelement() == 0:
                break
            # compute rl input vector
            offr_in = self.get_input_vector(
                i, rl_prev=rlact, sim_prev=simact, sim=False)
            # make a reinforcement learner offer
            rlact, clprobs = self.actor.step(offr_in)
            # update log probabilities
            self.lprobs[self.included, i] = clprobs
            # update delays and determine which threads concluded
            keeps, acc_rejs, timeouts = self.remove_threads(
                rlact, i, sim=False)
            # remove hidden states of concluded threads from internal state of actor and simulator
            # keeps is a byte tensor, not long tensor
            self.actor.shrink_hidden(keep_inds=keeps)
            self.sim.shrink_hidden(keep_inds=keeps)
            # update offer values
            self.update_offrs(
                rlact, i, sim=False)
            # compute rewards for most recent turn
            self.update_rewards(rlact, acc_rejs, timeouts, i, sim=False)
            # update rl and simulator action vectors to maintain only continuing indices
            rlact = rlact[keeps, :]
            if i != 0:
                simact = simact[keeps, :]
            # update included
            self.included = self.included[keeps]
            # check whether there are any continuing threads
            if self.included.nelement() == 0:
                break
            offr_in = self.get_input_vector(
                i, sim_prev=simact, rl_prev=rlact, sim=True)
            simact = self.sim.step(offr_in)
            keeps, acc_rejs, timeouts = self.remove_threads(
                simact, i, sim=True)
            # shrink hidden states as necessary
            self.actor.shrink_hidden(keep_inds=keeps)
            self.sim.shrink_hidden(keep_inds=keeps)
            # update offers
            self.update_offrs(simact, i, sim=True)
            # update rewards
            self.update_rewards(simact, acc_rejs, timeouts, i, sim=True)
            # update actions
            rlact = rlact[keeps, :]
            simact = simact[keeps, :]
            # update remaining included indices
            self.included = self.included[keeps]
        # handle final case where buyer can only accept or reject
        if self.included.nelement() != 0:
            # compute final input vector
            offr_in = self.get_input_vector(
                self.MAX_ROUNDS, rl_prev=rlact, sim_prev=simact, sim=False)
            # compute final set of actions
            aprobs = self.actor.final_step(offr_in)
            # compute rejections
            rejs = (aprobs < .5).byte()
            # compute rejection probabilities
            aprobs[rejs] = 1 - aprobs
            # store log probabilities
            self.lprobs[self.included, self.MAX_ROUNDS] = aprobs
            # find timeouts and acceptance/rejections
            _, acc_rejs, timeouts = self.remove_threads(
                rlact, self.MAX_ROUNDS, sim=False)
            # update offers then rewards
            self.update_offrs(rlact, self.MAX_ROUNDS, sim=False)
            self.update_rewards(rlact, acc_rejs, timeouts,
                                self.MAX_ROUNDS, sim=False)

    def update_offrs(self, act, acc_rejs, timeouts, turn, sim=False):
        """
        Computes the real value of the most recent offer in dollars

        Args:
            act:
            acc_rejs:
            timeouts:
            turn:
        Kwargs:
            sim:

        Returns:
            na
        """
        turn = turn * 2 + 1 if sim else turn * 2
        cont = torch.ones((acc_rejs.nelement())).byte()
        cont[timeouts] = 0
        # compute normalized value of all offers
        offr_vals = act[:, 2]
        offr_vals[acc_rejs] = act[acc_rejs, 1]
        # subset to those which didn't time out
        offr_vals = offr_vals[cont]
        # compute leftover values
        left_vals = 1 - offr_vals
        # compute indices to update
        update_inds = self.included[cont]
        if turn == 0:
            self.offrs[update_inds,
                       0] = self.init_prices[update_inds] * offr_vals
        elif turn == 1:
            self.offrs[update_inds, 1] = self.offrs[update_inds, 0] * \
                offr_vals + self.init_prices[update_inds] * left_vals
        else:
            self.offrs[update_inds, turn] = self.offrs[update_inds,
                                                       (turn - 1)] * offr_vals + \
                self.offrs[update_inds, (turn - 2)] * left_vals
        pass

    def update_rewards(self, act, acc_rejs, timeouts, turn, sim=False):
        """
        Updates rewards to include rewards from the most recent turn

        Args:
            act:
            acc_rejs:
            timeouts:
            turn:
        Kwargs:
            sim:

        Returns:
            NA
        """
        acc = torch.zeros(acc_rejs.nelement()).byte()
        rej = torch.zeros(acc_rejs.nelement()).byte()
        acc[acc_rejs] = act[acc_rejs, 1].byte()
        rej[acc_rejs] = act[acc_rejs, 1].byte() ^ 1
        prev_turns = torch.arange(turn + 1).long()
        if not sim:
            # for all turns where an offer is made (accept, reject, or otherwise add the cost of making an offer)
            self.rewards[self.included[acc_rejs],
                         prev_turns] = self.rewards[self.included[acc_rejs], prev_turns] + self.rl_params['offr_cost']
        # compute rewards for accepted offers
        offr_num = turn * 2 + 1 if sim else turn * 2
        # subtract amount paid
        self.rewards[self.included[acc], prev_turns] = self.rewards[self.included[acc]
                                                                    ] - self.offrs[self.included[acc], offr_num]
        # add original final sale amount
        self.rewards[self.included[acc], prev_turns] = self.rewards[self.included[acc]
                                                                    ] + self.sale_prices[self.included[acc]]
        pass

    def get_input_vector(self, turn, rl_prev=None, sim_prev=None, sim=False):
        """
        Computes the input vector for the upcoming turn. Format of input vector:
        - First s_o indices of reinforcement learner offer input vector consist of features
        generated by last seller (simulator) offer...currently:
            - 0 - indicator for whether a message is given
            - 1 - indicator for whether offer is 50 / 50
            - 2 - normalized value of offer
            - 3 - normalized buyer delay
            - 4 - real value of offer (rounded)
            - 5 - real value of delay (secs) (rounded?)
        - First s_1 indices of simulator offer input vector consist of features generated
        by the last seller (simulator) offer...currently: see above
        - Middle b_o indices of reinforcement learner offer input vector consist of features
        generated by the last buyer (rl) offer...currently: see above
        - Middle b_1 indices of simulator offer input vector consist of features
        generated by the last buyer (rl) offer...currently: see above
        - Last k_o indices of reinforcement learner offer input vector consist of time
        valued features
        - Last k_1 indices of simulator offer input vector consist of time valued features

        Args:
            turn:
        Kwargs:
            rl_prev:
            sim_prev:
            sim:

        Returns:
            None
        """
        # compute offer number associated with the upcoming turn
        offr_num = turn * 2 + 1 if sim else turn * 2
        # compute offer number associated with previous two turns
        # additionally compute number of time valued features to be included
        if sim:
            rl_prev_num = offr_num - 1
            sim_prev_num = offr_num - 2
            num_time = self.rl_params['n_timeval']
        else:
            rl_prev_num = offr_num - 2
            sim_prev_num = offr_num - 1
            num_time = self.rl_params['n_timeval']
        # compute total number of features to be included
        num_feats = self.PREV_FEATS * 2 + num_time + 1
        # initialize null output vector
        out = torch.zeros((self.included.nelement(), num_feats)).float()

        # compute features related to previous rl (buyer) turn
        if rl_prev is None:
            rl_prev = torch.zeros(
                (self.included.nelement(), self.PREV_FEATS)).float()
        else:
            rl_prev = self.prev_to_input(rl_prev, rl_prev_num)

        # compute features related to previous simulator (seller) turn
        if sim_prev is None:
            sim_prev = torch.zeros(
                (self.included.nelement(), self.PREV_FEATS)).float()
        else:
            sim_prev = self.prev_to_input(sim_prev, sim_prev_num)
        # compute time valued features
        time_feats = torch.from_numpy(self.listing_env.query_time(
            self.included.numpy(), self.delays.numpy(), sim=sim)).float()
        # add previous simulator turn features to input vector
        out[:, :self.PREV_FEATS] = sim_prev
        # add previous rl turn features to input vector
        out[:, self.PREV_FEATS:(self.PREV_FEATS * 2)] = rl_prev
        # add time valued features to input vector
        out[:, (self.PREV_FEATS * 2):(num_feats - 1)] = time_feats
        # set last feature to turn number
        out[:, num_feats - 1] = turn_num
        return out

    def prev_to_input(self, prev, offr_num):
        """
        Compute components of input vector corresponding to one of
        the two most recent turns, for which actions are given
        by prev

        Assumes the real offer value of prev has already been computed
        in ActorCriticEnvironment.offrs

        Args:
            prev:
            offr_num: integer denoting the offer number associated with
            previous turn given by prev

        Return:
        """
        out = torch.zeros((self.included.nelement(), self.PREV_FEATS))
        out[:, :4] = prev[:, :4]
        out[:, 4] = self.offrs[self.included, offr_num]
        out[:, 5] = self.delay_tracker[self.included, offr_num]
        return out

    def __get_params(self):
        '''
        Loads experiment parameters and simulator parameters from .jsons

        Initializes self.rl_params and self.sim_params dictionaries
        '''
        # open rl parameters file
        with open("data/rl/%s/parameters.json" % self.exp_name) as f:
            self.rl_params = json.load(f)
        # extract simulator name as a property
        self.sim_name = self.rl_params['sim_name']
        # open simulator parameters file
        with open("data/sims/%s/parameters.json" % self.sim_name) as f:
            self.sim_params = json.load(f)

    def __init_simulator(self):
        '''
        Extracts transition probability model parameters, stores them in an
        instance variable, then initializes transition probability model using
        these params, and updates parameters to values stored at the end of training

        Args:
            None

        Returns:
            None
        '''
        # extract parameters from name of trans_probs model and data loaded earlier
        Net = simulator_map()[self.sim_params['model_name']]
        self.sim = Net(self.sim_params['n_fixed'],
                       self.sim_params['n_hidden'],
                       self.sim_params['n_offr'],
                       self.sim_params['n_layer'])

        # parse location of model weights from trans_name
        weight_loc = 'data/sims/%s/model.pth.tar' % self.rl_params['sim_name']
        # load weights into Python
        model_dict = torch.load(weight_loc)
        # warm start trans model
        self.sim.load_state_dict(model_dict)
